---
title: "Logistic Regression"
subtitle: "Data Science for Biologists, Spring 2020"
author: "Poonam Shah"
output: 
  html_document:
    highlight: tango
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom) 
library(pROC)
library(patchwork)


set.seed(147)
```

## Instructions

Standard grading criteria apply. Make sure you set your seed, and **proofread to submit YOUR OWN WORDS!!**

This assignment will use an external dataset of various physical measurements from 752 adult Pima Native American women, some of whom are type II diabetic and some are not. Our goal for this assignment is to build and evaluate a model from this data to **predict whether an individual has Type II Diabetes** (column `diabetic`).

```{r, collapse=T}
pima <- read_csv("https://raw.githubusercontent.com/sjspielman/datascience_for_biologists/master/data/pima.csv")
dplyr::glimpse(pima)

# Not severely imbalanced. ROC will be fine.
pima %>% 
  count(diabetic)
```

Columns include:

+ `npreg`: number of times pregnant
+ `glucose`: plasma glucose concentration at 2 hours in an oral glucose tolerance test (units: mg/dL)
+ `dpb`: diastolic blood pressure (units: mm Hg)
+ `skin`: triceps skin-fold thickness (units: mm)
+ `insulin`: 2-hour serum insulin level (units: Î¼U/mL)
+ `bmi`: Body Mass Index
+ `age`: age in years
+ `diabetic`: whether or not the individual has diabetes


## Part 1: Build THREE models


First, you will build THREE distinct models (without stepwise model selection - just build them!) from the full data set.

1. The first model should include *all columns as predictors* (except `diabetic`, of course)


2. The second model should include these two predictors ONLY:
  + `glucose`
  + `bmi`

3. The third model should include these three predictors ONLY (same as #2, but with `skin` added into the mix):
  + `glucose`
  + `bmi`
  + `skin`

**Before you begin**, modify the `diabetic` column so that **Yes** is a "success" and **No** is a "failure." Rather than recoding as 1/0, instead let's just make the column a factor *with levels ordered Yes, No*! This will ensure "Yes" (has diabetes) is a success in the model.


```{r}
## Re-factor the diabetic column
## It is OK to overwrite the original pima dataset and the original diabetic column!

pima$diabetic <- factor(pima$diabetic, levels = c("No", "Yes"))

## Show the output to prove to yourself that the factoring worked
pima
```

```{r}
## Build model 1
model1<- glm(diabetic ~ ., data = pima, family="binomial")

## Build model 2 
model2<- glm(diabetic ~  glucose + bmi, data = pima, family="binomial")

## Build model 3 
model3<- glm(diabetic ~  glucose + bmi + skin, data = pima, family="binomial")

```


## Part 2: Compare the three models from part 1

+ In the space below, determine the AUC associated with each model. 

```{r}
## Run roc() on each fitted model
model1_roc <- roc(pima$diabetic, model1$fitted.values)

model2_roc <- roc(pima$diabetic, model2$fitted.values)

model3_roc <- roc(pima$diabetic, model3$fitted.values)
## Reveal the three models' AUC values here

model1_roc$auc
model2_roc$auc
model3_roc$auc

```


+ Then, plot an ROC curve of all three models in the same panel:

  + Model curves should be distinguished by color
  + Ensure that the models are *ordered* from best to worst in the legend, using AUC to determine this ranking. 
  + Models should be named in the legend as *Model X (AUC=...)*. For example, imagine model 1 has an AUC of 0.7 - it should appear in the legend as *Model 1 (AUC = 0.7)*. HINT: Just put whatever the appropriate phrase is as the name of the model when wrangling!! 
  + **Do not hardcode AUC values.** Always use a VARIABLE. To specifically get the auc value, do something like `roc_output$auc[[1]]` (the two square brackets!).
  
```{r}
## Plot the ROC curves below (which includes some wrangling!!)

tibble(FPR = 1 - model1_roc$specificities, 
       TPR=model1_roc$sensitivities, 
       model= paste0("Model 1(AUC =", round(model1_roc$auc[[1]], 3), ")" ), 
       auc = model1_roc$auc[[1]]) -> model1_tibble

tibble(FPR= 1 - model2_roc$specificities, 
       TPR=model2_roc$sensitivities, 
       model= paste0("Model 2(AUC =", round(model2_roc$auc[[1]], 3), ")" ), 
       auc = model2_roc$auc[[1]]) -> model2_tibble

tibble(FPR= 1- model3_roc$specificities, 
       TPR=model3_roc$sensitivities, 
       model= paste0("Model 3(AUC =", round(model3_roc$auc[[1]], 3), ")" ), 
       auc = model3_roc$auc[[1]]) -> model3_tibble

bind_rows(model1_tibble, model2_tibble, model3_tibble) %>%
  ggplot(aes(x = FPR, y = TPR, color = fct_reorder(model, auc, .desc = TRUE))) +
    geom_line()+
      geom_abline()+
      labs(color = "Models")

```


#### Part 2 Questions
Answer each in 1-2 sentences each *that are CLEARLY WRITTEN in YOUR OWN WORDS without ELEMENTARY-SCHOOL-LEVEL TYPOS.* We! Are! In! College!

1. Which model has the highest AUC value, and what is its AUC? Given that AUC can range 0.5-1, do you believe this is highly accurate model?

  + The highest AUC is Model 1, 0.8385

2. Compare models 2 and 3. What are the AUC values for each? Does including `skin` in the model seem to improve the model's performance?

  + The AUC of Model 2 is 0.8186. The AUC od Model 3 is 0.8198. There was a little bit imporvement by added skin. It didn't make much of a different

## Part 3: Work with the best model

Determine the best model (highest AUC) from Parts 1 and 2, and use this model for Part 3. Perform the following tasks:

+ Evaluate it with a training and testing split, where *75%* of the data is in the training split. Do NOT hardcode this 75% value!

```{r}
## Training and testing code goes here

training_percent <- 0.75

training_data <- sample_frac(pima, training_percent)
testing_data <- anti_join(pima, training_data)

#training split
train_model <- glm(diabetic ~ ., data = training_data, family ="binomial")
train_roc <- roc(training_data$diabetic, train_model$fitted.values)
train_roc$auc

#testing split
test_model <- glm(diabetic ~ ., data = testing_data, family ="binomial")
test_roc <- roc(testing_data$diabetic, test_model$fitted.values)
test_roc$auc

```


+ Plot the logistic regression curve for the training and testing models you build. This will be two plots, which you should add together with patchwork to display, OR use some wrangling skills to make a faceted plot in the first place. 
  + Do NOT!! color the logistic curve line
  + DO add colored points for individual observations along the curve
  + Rugs are optional
  
```{r}
## Plotting, and associated wrangling for plotting, goes here

#training data
tibble(x=train_model$linear.predictors, 
      y= train_model$fitted.values,
       diabetic = training_data$diabetic) %>%
  ggplot(aes(x=x,y=y)) +
  geom_line()+
  geom_point(aes(color = diabetic), alpha = 0.6, size = 0.75)+
  theme(legend.position = "bottom")+
  labs(x="Linear predictors", y= "Probabilty diabatic", title="Training result") -> train_plot


tibble(x=test_model$linear.predictors, 
      y= test_model$fitted.values,
       diabetic = testing_data$diabetic) %>%
  ggplot(aes(x=x,y=y)) +
  geom_line()+
  geom_point(aes(color = diabetic), alpha = 0.6, size = 0.75)+
  theme(legend.position = "bottom")+
  labs(x="Linear predictors", y= "Probabilty diabatic", title="Testing result") -> test_plot

train_plot + test_plot
```  


+ Determine the AUC value for each, and plot the ROC curves for training and testing in the space below. Again, use one plotting panel and distinguish the fits with colors.
```{r}
## ROC for training/testing goes here
#trainig data
train_data <- tibble(FPR = 1 - train_roc$specificities,
                     TPR = train_roc$sensitivities,
                     model = "Training split")
test_data <- tibble(FPR = 1 - test_roc$specificities,
                     TPR = test_roc$sensitivities,
                     model = "Testing split")

#testing data
bind_rows(train_data, test_data) %>%
  ggplot(aes(x=FPR, y=TPR, color= model)) +
  geom_line(aes(size = model)) +
  scale_size_manual(values = c(2,1))
```

#### Part 3 Question

How does this model perform when considering a training and testing split? Compare their specific AUC values and discuss whether you believe the model suffers from overfitting or not in 1-2 sentences.

+ The training and testing are split around the x= 0.25 y=0.75 points. It comes together at the end and the beginging of the plot. The trainging data for the roc is 0.8377. The testing data for the roc is 0.8578. 


## Part 4: Evaluating at a given threshold

Determine the following measures for the best model (the FULL fit from part 1, NOT a training/testing split from part 3!!) assuming a *success threshold of 0.8*. Refer to the slides for formulas!

+ Accuracy
+ False positive rate
+ Positive predictive value

```{r}
threshold <- 0.8
model1 <- glm(diabetic ~ ., data = pima, family = "binomial")

## Code to calculate goes here
tibble(prob_diabetic = model1$fitted.values,
       truth         = pima$diabetic,
       prediction    = if_else(prob_diabetic >= threshold, "Yes", "No")) %>%
  mutate(classif = case_when(truth == "Yes" & prediction == "Yes" ~ "TP",
                             truth == "No" & prediction == "No" ~ "TN",
                             truth == "Yes" & prediction == "No" ~ "FN",
                             truth == "No" & prediction == "Yes" ~ "FP",)) %>%
  count(classif) %>%
  pivot_wider(names_from = classif, values_from = n) -> final_results

final_results

# Accuracy:
#(final_results$TP + final_results$TN) / (final_results$TP + final_results$TN + final_results$FP = final_results$FN) = 0.715

final_results %>%
  mutate((TP + TN) / (TP + TN + FP + FN))

#FPR = 0.0246
final_results %>%
  mutate(FP/ (TN + FP))

#PPV = 0.838
final_results %>%
  mutate(TP/ (TP + FP))
```

## Part 5: Predicting outcomes

Determine the probability that these two new women have diabetes, again using the best model. Here is their data (choose which columns to include in your tibbles based on what's needed for the best model):

+ Female 1
  + `npreg`: 5
  + `glucose`: 110
  + `dpb`: 78
  + `skin`: 20
  + `insulin`: 58
  + `bmi`: 25
  + `age`: 26
+ Female 2
  + `npreg`: 3
  + `glucose`: 175
  + `dpb`: 92
  + `skin`: 28
  + `insulin`: 222
  + `bmi`: 38
  + `age`: 45


```{r}
## code to predict goes here
## make sure to predict PROBABILITIES!!! 
## your code MUST PRINT OUT the probabilities for each woman in the end

tribble(~npreg, ~glucose, ~dbp, ~skin, ~insulin, ~bmi, ~age,
        5,      110,       78,    20,    58,       25,   26,
        3,      175,       92,    28,    222,      38,   45 ) -> new_pima
predict(model1, new_pima, type= "response")
```

#### Part 5 Questions:

1. At a threshold of 80%, does the model predict that Woman 1 is diabetic? Answer yes or no!

  + no
  
2. At a threshold of 80%, does the model predict that Woman 2 is diabetic? Answer yes or no!

  + no
